{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unlimited-fever",
   "metadata": {},
   "source": [
    "<h1>Block algorthims using dask array</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-relay",
   "metadata": {},
   "source": [
    "Blocked algorithms allow you to process data too big to fit into memory.  Dask uses these to both tackle huge datasets and to parallelize the work.<br><br>\n",
    "Blocked mean example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = h5py.File('myfile.hdf5')['x']             #Trillion element array on disk\n",
    "sums = []                                     \n",
    "counts = []\n",
    "for i in range(1_000_000):                    #Loop through array 1 million times\n",
    "    chunk = x[1_000_000*i: 1_000_000*(i+1)]   #Pull out each chunk\n",
    "    sums.append(np.sum(chunk))                #Sum chunk\n",
    "    counts.append(len(chunk))                 #Count chunk\n",
    "    \n",
    "result=sum(sums)/sum(counts)                  #Aggregate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-asbestos",
   "metadata": {},
   "source": [
    "<br><br>Dask implements a subset of ndarray's api using blocked algorithms to distribute the data/processing \n",
    "across multiple ndarrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-edward",
   "metadata": {},
   "source": [
    "<br>Block algorthims like above can be split into parallel operations because there are no dependencies.  This allows you to process data that fits on disk, but maybe not in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-framework",
   "metadata": {},
   "source": [
    "<h3>Example Numpy ndarray operations</h3>\n",
    "\n",
    "Create a numpy ndarray and fill with random numbers to simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.random((10000, 10000))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size in GB:\",x.nbytes / 1e9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Perform some operations:\n",
    "y = x + x.T\n",
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-nirvana",
   "metadata": {},
   "source": [
    "<br><h3>Now let's do it in dask array</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dask array and create same random data, breaking into chunks.\n",
    "import dask.array as da\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Same operation, but lazy compute, so returns instantly\n",
    "y = x + x.T\n",
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-diagnosis",
   "metadata": {},
   "source": [
    "<br><h4>Let's try again, but with a 10 trillion elements</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-heath",
   "metadata": {},
   "source": [
    "By default dask uses a local threaded cluster which parallelizes operations and limits memory usage but is limited to a single core, so processor intensive applications won't see much performance improvement (IO bound ones will).<br>You can easily change this to a multi-process local cluster by importing the distributed package which scales from local machine, to ad hoc clusters, to cloud services to HPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import distributed and create a local 4 process client with restricted memory usage.  \n",
    "from dask.distributed import Client, progress\n",
    "client = Client(processes=True, threads_per_worker=4,\n",
    "                n_workers=2, memory_limit='1GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((100000, 100000), chunks=(1000, 1000))#10 trillion\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same operation, lazy compute\n",
    "y = x + x.T\n",
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-french",
   "metadata": {},
   "source": [
    "<h4>Dask arrays allow for reduced memory footprint and parallel processing of blocked algorithms</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Tab completion shows api implemented\n",
    "import dask.array as da\n",
    "da."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-vertex",
   "metadata": {},
   "source": [
    "Close down the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
