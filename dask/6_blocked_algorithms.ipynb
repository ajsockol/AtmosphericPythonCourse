{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incorrect-scope",
   "metadata": {},
   "source": [
    "<h1>Block algorthims using dask array</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-stanford",
   "metadata": {},
   "source": [
    "Blocked mean example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = h5py.File('myfile.hdf5')['x']             #Trillion element array on disk\n",
    "sums = []                                     \n",
    "counts = []\n",
    "for i in range(1_000_000):                    #Loop through array 1 million times\n",
    "    chunk = x[1_000_000*i: 1_000_000*(i+1)]   #Pull out each chunk\n",
    "    sums.append(np.sum(chunk))                #Sum chunk\n",
    "    counts.append(len(chunk))                 #Count chunk\n",
    "    \n",
    "result=sum(sums)/sum(counts)                  #Aggregate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-travel",
   "metadata": {},
   "source": [
    "<br><br>Dask implements a subset of ndarray's api using blocked algorithms to distribute the data/processing \n",
    "across multiple ndarrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absolute-means",
   "metadata": {},
   "source": [
    "<br>Block algorthims like above can be split into parallel operations because there are no dependencies.  This allows you to process data that fits on disk, but maybe not in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-extra",
   "metadata": {},
   "source": [
    "<h3>Example Numpy ndarray operations</h3>\n",
    "\n",
    "Create a numpy ndarray and fill with random numbers to simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.random((10000, 10000))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size in GB:\",x.nbytes / 1e9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-eight",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Perform some operations:\n",
    "y = x + x.T\n",
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-malaysia",
   "metadata": {},
   "source": [
    "<br><h3>Now let's do it in dask array</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dask array and create same random data, breaking into chunks.\n",
    "import dask.array as da\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size in GB:\",x.nbytes / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Same operation, but lazy compute, so returns instantly\n",
    "y = x + x.T\n",
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-attendance",
   "metadata": {},
   "source": [
    "<br><h4>Let's try again, but with a 10 trillion elements</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-hepatitis",
   "metadata": {},
   "source": [
    "By default dask uses a local threaded cluster which parallelizes operations and limits memory usage but is limited to a single core, so processor intensive applications won't see much performance improvement (IO bound ones will).<br>You can easily change this to a multi-process local cluster by importing the distributed package which scales from local machine, to ad hoc clusters, to cloud services to HPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import distributed and create a local 4 process client with restricted memory usage.  \n",
    "from dask.distributed import Client, progress\n",
    "client = Client(processes=True, threads_per_worker=1,\n",
    "                n_workers=4, memory_limit='1GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-lodge",
   "metadata": {},
   "source": [
    "When running on remote servers, you can use a ssh redirector to view the dashboard:<br>\n",
    "ssh nimbus3 -L 8787:localhost:8787\n",
    "\n",
    "and then in a browser:\n",
    "http://localhost:8787/status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((100000, 100000), chunks=(1000, 1000))#10 trillion\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same operation, lazy compute\n",
    "y = x + x.T\n",
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-invasion",
   "metadata": {},
   "source": [
    "<h4>Dask arrays allow for reduced memory footprint and parallel processing of blocked algorithms</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Tab completion shows api implemented\n",
    "import dask.array as da\n",
    "da."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-pontiac",
   "metadata": {},
   "source": [
    "Close down the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
