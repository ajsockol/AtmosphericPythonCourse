{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "micro-james",
   "metadata": {},
   "source": [
    "<h1>Dask workers - the cluster</h1>\n",
    "Dask uses a cluster of workers to perform operations. \n",
    "\n",
    "<h4>Cluster types</h4>\n",
    "<ul><li>By default, if you don't specify, Dask creates a local (same pc) cluster of multiple threads</li></ul>\n",
    "Threads are subprocesses (share same core & memory space), fast to create and manage and good for general tasks, mostly io.\n",
    "<ul><li><h4>Dask distributed</h4> is an enhanced cluster manager that can create threaded and multiprocess clusters on your locahost, a cluster of ssh connected servers, a cloud provider and a HPC like Orion</li></ul>\n",
    "<b>Use distributed when scaling up a cluster from the default.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-triumph",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "#or more explicityly\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-overall",
   "metadata": {},
   "source": [
    "This creates a multiprocess local cluster, perfect for general use without having to tune.<br>Always clean up by explicitly shutting down the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-damage",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-center",
   "metadata": {},
   "source": [
    "Distributed clusters create a dashboard to visualize activity.  This is at port 8787 by default, or a random port if that is taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-cleanup",
   "metadata": {},
   "source": [
    "The scheduler port is used by workers to cooridinate activity. The dashboard port is on the machine <b>where the code is run from</b>.<br><br>\n",
    "If <b>running on your laptop</b>, you can just open url http://localhost:[port]<br><br>\n",
    "If you have ssh'd into <b>GML servers</b> you will need to use <b>port redirection</b> to access the dashboard on the remote server:<br>\n",
    "ssh [server] -L (local redirect) [localport]:[remote host]:[remote port]\n",
    "\n",
    "ssh nimbus3 -L 8787:localhost:8787\n",
    "\n",
    "and then in a browser: http://localhost:8787/status\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-reader",
   "metadata": {},
   "source": [
    "<h4>You can also use Jupyter Lab to create the cluster and hook it in for you</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-tiffany",
   "metadata": {},
   "source": [
    "You can tune the local cluster, specifying specific details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client(processes=True, threads_per_worker=1,\n",
    "                n_workers=4, memory_limit='1GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-advocacy",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-complaint",
   "metadata": {},
   "source": [
    "<h4>SSHCluster</h4> creates workers on ssh connected machines, creating a cluster of local servers.  This requires all servers to have ssh key exchange set up and a consistent python environment.  Our GML servers can do this easily with NFS mounts and one time key setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The nimbi cluster:\n",
    "from dask.distributed import Client, progress, SSHCluster\n",
    "cluster = SSHCluster(['nimbus4','nimbus','nimbus2','nimbus3'],    #First server is the scheduler\n",
    "     scheduler_options={\"dashboard_address\": \":8884\"},            #Specify a specific port so you can set up redirector\n",
    "     worker_options={'nprocs': 2,'nthreads': 1})                  #Specify the number of workers per host machine\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-quantum",
   "metadata": {},
   "source": [
    "<h2>With great power comes great responsibility!</h2>This can easily overwhelm the servers, bringing the wrath of IT down upon you.<br>Use these sparingly, in off hours.  Coordinate with IT for large jobs.<br>\n",
    "<b>Limit to no more than 2 processes & 2 threads per host.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-spanking",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-banana",
   "metadata": {},
   "source": [
    "<h4>Make sure to shutdown cleanly</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-baptist",
   "metadata": {},
   "source": [
    "<h3>dask_jobqueue</h3>Lets you create and distribute jobs on nodes of a HPC.<br>\n",
    "SLURMCluster can be used to submit jobs on the Orion HPC which uses the SLURM workload manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "from dask_jobqueue import SLURMCluster\n",
    "slurmextra=['-p orion','-q batch','--mail-user=john.mund@noaa.gov','--time=00:05:00']\n",
    "cluster=SLURMCluster(project='co2',cores=6,processes=6,memory='1GB',log_directory='./logs',job_extra=slurmextra)\n",
    "cluster.scale(jobs=3)\n",
    "client = Client(cluster)\n",
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-istanbul",
   "metadata": {},
   "source": [
    "This allows you to seemlessly scale code from local cluster on your laptop, to MP cluser, to SSH cluster to HPC or cloud based hosts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
