{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "advised-floor",
   "metadata": {},
   "source": [
    "<h1>Reading ERA5 data with xarray & Dask</h1>\n",
    "Extract data from 12 monthly files, then process selected dataset for target lat/lon.  Each file is ~2gb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import datetime as dt\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "csvpath=\"output/era5/\";outfile=\"\";yr=2018\n",
    "sitelon = 40.02 #example for Boulder, CO\n",
    "sitelat = -105.27 #example for Boulder, CO\n",
    "\n",
    "#Adj target lon to be on same scale as data\n",
    "sitelon=sitelon%360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tricks to get around OS file cacheing so we can see actual read data performance...\n",
    "#These are all pointing to the same files.\n",
    "#e5path=\"/work/noaa/co2/kaushik/PVPRM/ERA5_PVPRM\"#orion\n",
    "e5path1='/home/ccg/mund/tmp/lns/e1/'\n",
    "e5path2='/home/ccg/mund/tmp/lns/e2/'\n",
    "e5path3='/home/ccg/mund/tmp/lns/e3/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-missouri",
   "metadata": {},
   "source": [
    "<h3>Possible approaches</h3>\n",
    "<ul>\n",
    "<li>Loop through each monthly file, exctract data and write into intermediate data structure\n",
    "<li>Use xarray's open_mfdataset to combine files into 1 logical structure (41GB!), extract data and process.  This actually uses dask behind the scenes and runs in multi-threaded mode, so it's memory effecient and fairly fast    \n",
    "<li>Use xarray's open_mfdataset in parallel processing mode\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-arnold",
   "metadata": {},
   "source": [
    "<h4>Looping</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove this line to run... ~5-6 min\n",
    "%%time\n",
    "#Traditional looping method: \n",
    "outfile=\"looping_output.csv\"\n",
    "files=glob.glob(e5path1 + 'era5_daily_pvprm_%s*' %(yr))\n",
    "n=0;frames=[]\n",
    "for f in files:\n",
    "    t=xr.open_dataset(f)\n",
    "    tloc = t.sel(longitude=sitelon, latitude=sitelat, method ='nearest')\n",
    "    frames.append(tloc)\n",
    "looploc=xr.combine_by_coords(frames,combine_attrs='drop')\n",
    "looploc\n",
    "\n",
    "#CPU times: user 636 ms, sys: 1.4 s, total: 2.03 s\n",
    "#Wall time: 5min 13s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-clear",
   "metadata": {},
   "source": [
    "<h4>Multi-threaded using Xarray open_mfdataset</h4>This uses Dask underneath with a multi-threaded cluster by default.  It is memory efficient, a cleaner algorithm and a little bit faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove this line to run... ~4 min\n",
    "%%time\n",
    "outfile=\"mt_output.csv\"\n",
    "dsm = xr.open_mfdataset(e5path2 + 'era5_daily_pvprm_%s*' %(yr))\n",
    "\n",
    "# subset location, put into dataframe \n",
    "dsmloc = dsm.sel(longitude=sitelon, latitude=sitelat, method ='nearest').compute()\n",
    "#dsmloc\n",
    "\n",
    "#CPU times: user 566 ms, sys: 1.44 s, total: 2 s\n",
    "#Wall time: 4min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-diploma",
   "metadata": {},
   "source": [
    "<h4>Multi-process using Xarray open_mfdataset</h4>\n",
    "Use dask explicitly to create a multi-processing cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress, SSHCluster\n",
    "#client = Client()#Default mp cluster\n",
    "client=Client(address='nimbus2:8750')#ssh nimbi cluster, started from command line\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#CPU times: user 11.9 s, sys: 1.22 s, total: 13.1 s\n",
    "#Wall time: 2min 56s\n",
    "outfile=\"mp_output.csv\"\n",
    "dsm = xr.open_mfdataset(e5path3 + 'era5_daily_pvprm_%s*' %(yr),parallel=True,chunks={'latitude': 10, 'longitude': 10})\n",
    "dsmloc = dsm.sel(longitude=sitelon, latitude=sitelat, method ='nearest').compute()\n",
    "#dsmloc\n",
    "\n",
    "#CPU times: user 854 ms, sys: 0 ns, total: 854 ms\n",
    "#Wall time: 27.4 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DSM size in GB:\",dsm.nbytes / 1e9)\n",
    "print(\"DSMloc size in GB:\",dsmloc.nbytes / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-charm",
   "metadata": {},
   "source": [
    "<h3>Process selected data into output</h3>\n",
    "Filter data using Dask, then use Numpy, Pandas & Xarray to process.  This is fast because we're now working on a relatively small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# do some math/edit the arrays \n",
    "\n",
    "dfmloc=dsmloc.to_dataframe()\n",
    "\n",
    "# avg soil data, convert stl1+stl2 to avg and append\n",
    "soilt=(dfmloc.stl1+dfmloc.stl2)/2.\n",
    "dfmloc['Ts.ERA5']=soilt\n",
    "\n",
    "# drop stl1, stl2, lat, lon\n",
    "dfmloc2=dfmloc.drop(['ssrd','stl1','stl2','latitude','longitude'],axis=1)\n",
    "#del dfmloc\n",
    "\n",
    "# rename t2m,par into new dataframe\n",
    "dfmloc3=dfmloc2.rename(columns={\"t2m\":\"Ta.ERA5\",\"par\":\"PAR.ERA5\"})\n",
    "#del dfmloc2\n",
    "\n",
    "# convert Ta and Ts to Celsius\n",
    "dfmloc3['Ta.ERA5']=dfmloc3['Ta.ERA5']-273.15\n",
    "dfmloc3['Ts.ERA5']=dfmloc3['Ts.ERA5']-273.15\n",
    "# new time index going up to 23:30 on 12/31 -- era5 data ends at 23:00\n",
    "t_index=pd.date_range(start='%s-01-01 00:00:00' %(yr), end='%s-12-31 23:30:00' %(yr), freq='30T')\n",
    "\n",
    "#resample on 30min timestep, interpolate (default linear?), reindex to t_index to go up to 23:30, /\n",
    "#forward fill last value, reset index so that time is one  of the columns\n",
    "dfmloc_rs=dfmloc3.resample('30T').interpolate().reindex(t_index).ffill().reset_index()\n",
    "\n",
    "# save to dataframe, rename index column to time\n",
    "dfm=dfmloc_rs.rename(columns={\"index\":\"time\"})\n",
    "#del dfmloc_rs\n",
    "\n",
    "#Output to csv\n",
    "dfm.to_csv(csvpath+outfile,index=False)\n",
    "#del dfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-access",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
